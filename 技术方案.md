# 飞书合同协同记录批量拉取 技术方案

## 1. 概述
- 目标：在无法修改前台、单次导出最多 1000 条的约束下，自动化批量导出 2022 年至今的协同记录，最终落地为 Excel，并提供进度与预计完成时间显示，支持自动化自检与验收。
- 关键约束：
  - 导出接口单次上限 1000 条。
  - 无法通过前端直接下载，需要在飞书网页版消息中点击“下载文件”。
  - 已掌握导出接口 curl，可按时间范围筛选。

## 2. 需求回顾（来自《需求文档》）
- 时间范围：2022-01-01 至今。
- 存储：落地为 Excel 文件，并支持后续合并。
- 下载：通过浏览器自动化访问 https://li.feishu.cn/next/messenger 扫码登录，进入“飞书合同”机器人会话，点击“下载文件”。
- 日志：展示导出进度、预计完成时间等信息（无需外部通知）。
- 验收：具备自动化测试能力，自检最终 Excel 字段与数据一致性。

## 3. 设计原则与假设
- 使用脚本自动发起“按时间窗口”的导出任务；若某窗口数据量接近或超过上限，自动缩小窗口（7d→3d→1d）。
- 导出接口调用携带实时抓取的 Header（csrf-token、Cookie 等），不做长期 cookie 缓存；浏览器自动化会话可使用临时或持久用户目录以减少重复扫码。
- 严格控制并发与重试，遵守服务限流。

## 4. 总体架构
- 模块划分：
  - 配置模块：时间范围、窗口粒度、并发数、下载目录、浏览器配置等。
  - 调度与时间窗口模块：生成窗口、失败重试、断点续跑。
  - 导出调用模块：封装 HTTP 请求，提交导出任务。
  - 下载自动化模块：Playwright 打开飞书网页版，扫码登录，定位机器人消息并点击下载。
  - 文件管理与合并模块：命名、合并、校验。
  - 日志与进度模块：进度百分比、速率、ETA、关键事件。
  - 测试与验收模块：字段校验、缺失检测。

## 5. 端到端流程
1) 初始化配置与任务状态（JSON/SQLite）。
2) 生成 7 天为步长的初始时间窗口列表。
3) 逐窗口调用导出接口，记录任务发起时间与参数。
4) 启动浏览器自动化：
   - 访问 https://li.feishu.cn/next/messenger → 扫码登录 → 进入“飞书合同”机器人会话。
   - 捕获“飞书合同 - 协商操作记录导出完成”消息，并解析文案中的“协商数据（共计：XXX）”；若 XXX == 1000，则标记该时间窗口需细分；随后点击“下载文件”，拦截下载保存到指定目录并按规则重命名。
   - 为该时间窗口设置消息等待超时：max_wait_seconds（默认 90s，且不小于 1000 条记录消息的平均接收时间）；若超时仍未收到消息，则视为“无协同记录”，标记窗口为 no_data 并跳过下载。
   - 若消息“共计：XXX”==1000，则按拆分序列 [7,3,1] 逐步缩小该时间窗口，直至“共计”<1000 且完成下载；仅在下载成功后，才将该窗口记录为 with_data 并写入 CSV。
   - 若窗口已缩小至 1 天且“共计：XXX”仍为 1000，则记为 manual（异常），在 CSV 中标注 exception=over_limit_1d 与 window_days=1，后续由人工处理；流程继续推进到下一天。
5) 对每个下载完成的文件执行快速校验：
   - 仅依据机器人消息中的“共计”数进行判断；若为 1000，标记该窗口需细分并拆分为更小子窗口重新导出。
6) 当所有窗口均低于上限且成功下载，执行合并，并产出汇总文件与校验报告。
7) 输出最终统计、日志与验收结果。

## 6. 时间窗口切分算法
 - 初始窗口：7 天。
 - 自适应：依据机器人消息“共计”数判断是否需要细分（阈值 1000），按 7d→3d→1d 递进。
 - 拆分序列：split_days_sequence = [7, 3, 1]。
 - 接受条件：子窗口“共计”<1000 且下载成功，才视为该窗口合格并写入 CSV。
 - 推进规则：子窗口合格后，从 next_start = accepted_end + 1 天起继续覆盖原始区间，直至覆盖到原始 end。
 - 终止条件：若已为 1 天且“共计”==1000，则标记 manual（exception=over_limit_1d，window_days=1），并推进到下一天。
- 不做去重：导出数据保留原样；通过“推进规则”避免时间区间重叠导致的重复覆盖。
- 断点续跑：持久化每个窗口状态（pending/success/failed/split），支持失败重试与跳过已完成窗口。

## 7. 接口与自动化实现
### 7.1 导出接口
- URL：POST https://contract.feishu.cn/clm/api/cooperation/exportCooperationRecords
- Headers：
  - Timezone-Offset: -480
  - csrf-token: $CSRF_TOKEN
  - Cookie: session=...; sl_session=...; clm_csrf_token=...
  - Content-Type: application/json
- Body 示例：
```json
{
  "keyword": "",
  "searchTabEnumCode": 0,
  "searchCooperationByCreateTime": ["2023-01-01", "2023-12-01"]
}
```
- 说明：Headers 与 Cookie 由浏览器抓取后即时注入运行，不做长期持久化。

### 7.2 浏览器自动化下载
- 技术选型：Playwright。
- 关键步骤：
  1. 打开 https://li.feishu.cn/next/messenger，等待二维码 → 扫码登录。
  2. 首次运行定位“飞书合同”机器人会话并固定到侧边栏，后续直接进入。
  3. 在会话中找到最新“导出完成”消息，点击“下载文件”。
  4. 统一下载目录与命名：`合同协同_YYYYMMDD-YYYYMMDD_共{COUNT}条.xlsx`（目录相对当前代码工作区根目录）。
  5. 对下载完成事件（download.finished）注册回调，记录路径并触发后续校验。
  6. 消息等待：为每个时间窗口设置最大等待时长 max_wait_seconds（默认 90s，且不小于 1000 条记录消息的平均接收时间）；若超时未收到消息，则判定该窗口无数据并记录日志。
- 选择器策略：优先使用可读文案如 `text="下载文件"`，辅以 aria/role/contains，多层兜底；必要时滚动加载历史消息并按时间筛选最新。
- 会话持久化：可配置 `user_data_dir` 保存登录态以减少重复扫码（仅用于浏览器，非导出接口）。

### 7.3 合并与统计规则

- 合格窗口：仅 `status=with_data` 参与合并与统计。
- 跳过窗口：`no_data/manual/failed/skipped` 不计入合并的内容与总条数，但会保留在 CSV 以备审计与后续处理。
- 总条数 {TOTAL}：汇总 CSV 中所有 `with_data` 的 `declared_count` 之和；若某窗口缺失 `declared_count`，按 0 处理并记录警告。
- 文件命名：最终合并文件名使用 `output_path_pattern` 渲染，其中 `{TOTAL}` 为上述统计结果。
- 去重策略：不做去重，合并时保留原始导出内容。
- 审计输出：合并完成后生成统计摘要（窗口数、参与合并窗口数、TOTAL、manual/no_data/failed 数量）。

## 8. 配置项（config.yaml 示例）
```yaml
start_date: "2022-01-01"
end_date: "2025-12-31"
initial_window_days: 7
min_window_days: 1
split_days_sequence: [7, 3, 1]
max_count_per_file: 1000
max_concurrency: 2
retry:
  max_attempts: 3
  backoff_seconds: 5
export_headers:
  timezone_offset: -480
  csrf_token: ""  # 从配置文件直接填写当前抓包的值
  cookie: ""      # 从配置文件直接填写当前抓包的值
  content_type: "application/json"
download:
  driver: "playwright"
  user_data_dir: "./.browser_profile"  # 可选
  download_dir: "./output/raw"  # 相对当前代码工作区根目录
  bot_chat_name: "飞书合同"
  max_wait_seconds: 90  # 不小于1000条消息平均接收时间
merge:
  output_path_pattern: "./output/merged/合同协同_2022至今_共{TOTAL}条.xlsx"
run_state:
  csv_path: "./state/run_windows.csv"
  encoding: "utf-8-sig"       # 确保 Excel 正常解析
  line_ending: "crlf"
  flush: true                  # 逐条落盘
  resume_mode: "resume"       # resume|full，默认断点续跑
  completed_statuses: ["with_data", "no_data", "manual"]  # 视为已完成不再重跑
log:
  level: "INFO"
```

## 9. 日志与进度/ETA
- 指标：
  - 总窗口数/已完成数/失败数。
  - 平均每窗口耗时、最近 N 窗口移动平均耗时。
  - ETA = (剩余窗口数 × 移动平均耗时)。
- 关键事件：导出提交、下载完成、校验结果、窗口细分、重试、最终合并完成。
- 产物：按天生成滚动日志，支持 JSON 行便于后续分析。

### 9.1 运行窗口状态CSV（独立模块）

- 目标：以 CSV 持久化每个时间窗口的最终状态与度量，支持断点续跑与动态调整时间范围。
- 文件属性（独立于日志模块）：
  - 路径：`./state/run_windows.csv`
  - 编码：`utf-8-sig`（确保 Excel 正常解析）
  - 换行：CRLF（Windows 推荐）
  - 写入方式：追加写入，逐条落盘，写入后立即 flush。
- 写入时机（仅在“最终状态”明确后写入一条记录）：
  - with_data：文件“下载成功”且基础校验通过后写入。
  - no_data：达到 `max_wait_seconds` 未收到消息后写入。
  - skipped：因去重/被拆分覆盖等原因跳过后写入。
  - failed：重试穷尽后写入，用于后续重跑。
  - manual：窗口已为 1 天且“共计”==1000，无法再细分时写入，供人工处理。
- 字段建议（列头）：
  - `window_id`，`from_date`，`to_date`，`window_days`，`status`（with_data/no_data/skipped/failed/manual），
  - `declared_count`（消息“共计”数），`split_level`，`retries`，
  - `skip_reason`，`exception`（如 over_limit_1d），`file_path`，`file_md5`，
  - `start_time`，`end_time`，`duration_ms`。
- 示例（示意）：
```
window_id,from_date,to_date,window_days,status,declared_count,split_level,retries,skip_reason,exception,file_path,file_md5,start_time,end_time,duration_ms
20230101-20230103,2023-01-01,2023-01-03,3,with_data,842,1,0,,,output/raw/合同协同_20230101-20230103_共842条.xlsx,3a1c...,2025-11-12T10:00:00+08:00,2025-11-12T10:02:33+08:00,153000
20230104-20230107,2023-01-04,2023-01-07,4,no_data,0,1,0,timeout>90s,,,
20230108-20230108,2023-01-08,2023-01-08,1,manual,1000,2,0,,over_limit_1d,,,
```
- 断点续跑与全量重跑：
  - 由 `run_state.resume_mode` 控制：`resume`（默认）或 `full`。
  - resume：启动时读取 CSV，基于唯一 `window_id` 与 `completed_statuses` 构建“已完成窗口集”，仅对缺失/failed/manual 执行；避免重复执行已完成窗口。
  - full：忽略 CSV 的完成记录，重新生成全量窗口执行（建议先备份 CSV）。
  - 同一 `window_id` 如出现多行，以最后一行（最新状态）为准。

#### 字段约定

- `window_id`：`YYYYMMDD-YYYYMMDD`，与 `from_date/to_date` 对应；保证在全局唯一。
- `window_days`：`to_date - from_date + 1` 的天数。
- `split_level`：使用的拆分深度，0=原7天，1=3天，2=1天（对应 `split_days_sequence` 的索引）。
- `declared_count`：来自机器人消息“协商数据（共计：XXX）”。

- `exception`：异常原因枚举，例如 `over_limit_1d`、`download_timeout`、`schema_mismatch` 等。

### 9.2 状态机与断点续跑流程

- 状态机：
  - pending → waiting_message → with_data | no_data | manual | failed
  - with_data → merged（仅用于统计，不写入 CSV 的新状态）
  - split：当 `declared_count==1000` 触发，生成子窗口（3天/1天），父窗口不直接写入，子窗口按上述状态机推进。
- 触发条件：
  - with_data：下载成功且校验通过。
  - no_data：超过 `max_wait_seconds` 未收到任何消息。
  - manual：窗口已为1天但 `declared_count==1000`。
  - failed：达到最大重试仍异常（网络、选择器、解析等）。
 - 读取 `run_state.csv_path`，过滤 `completed_statuses`（默认 with_data/no_data/manual）。
  - `resume_mode=resume`：仅对缺失/failed 的 `window_id` 进行执行；
  - `resume_mode=full`：忽略完成记录，全部窗口重新执行。

## 10. 文件与目录
- 建议目录结构：
```
./
  config/
    config.yaml
  scripts/
    export_runner.py
    web_download.py
    merge_and_validate.py
  output/
    raw/  # 分窗口原始 Excel
    merged/
  logs/
  state/
    run_windows.csv
    windows_state.json
```

以上路径均相对当前代码工作区根目录。

## 11. 异常处理与重试
- HTTP 4xx/5xx：分类重试（指数退避），超过阈值标记失败。
- CSRF/Cookie 失效：提示更新 Header；不自动长期缓存。
- 浏览器自动化：登录失败/选择器失效/下载超时，自动刷新页面与重试，必要时人工兜底。
- 文件冲突：同名文件自动追加后缀与窗口标识，避免覆盖。
 - 消息等待超时：超过 max_wait_seconds 未收到任何导出消息，则视为该时间窗口无协同记录（no_data），记录窗口与时间戳并继续后续窗口处理。
 - CSV 流式写入失败：写入失败时将记录缓存在内存队列/`state/run_windows.csv.pending`，后台重试落盘；成功后删除临时文件，保证同一 `window_id` 最终仅一条有效记录。

## 12. 自动化测试与验收
 - 单元测试：
  - 时间窗口生成与细分逻辑。
  - 文件命名与合并策略。
- 集成测试：
  - 使用假数据/本地模拟下载事件验证合并与校验流程。
- 自检规则（针对最终 Excel 或合并结果）：
  - 字段完整性：必填列齐全、列名正确。
  - 数据一致性：日期范围、空值占比阈值等。
  - 阈值校验：若机器人消息“共计”=1000 则触发时间窗口细分与重跑。
- 验收标准对齐《需求文档》：无人值守完成、失败可重试、日志完备、抽查一致、具备自动化自检。


## 14. 风险与应对
- 频控或封禁：控制并发，设置间隔，与平台沟通白名单。
- 登录与页面变更：监控选择器并预留人工兜底；及时更新脚本。
 - 数据截断：消息“共计”阈值触发窗口细分并重跑；不做去重，依靠推进规则避免重复覆盖。

## 15. 运行前置清单
- Python 3.10+ 环境。
- 依赖：requests、pandas、openpyxl、playwright 或 selenium、pyyaml 等。
- 首次使用 Playwright 请执行 `playwright install` 安装浏览器内核。
- 更新 config.yaml 中的 csrf-token 与 Cookie 占位值（运行前抓取最新）。

## 16. 附：示例请求（占位）
```bash
curl --location 'https://contract.feishu.cn/clm/api/cooperation/exportCooperationRecords' \
--header 'Timezone-Offset: -480' \
--header 'csrf-token: $CSRF_TOKEN' \
--header 'Cookie: $COOKIE' \
--header 'Content-Type: application/json' \
--data '{
  "keyword": "",
  "searchTabEnumCode": 0,
  "searchCooperationByCreateTime": ["$FROM_DATE", "$TO_DATE"]
}'
```
